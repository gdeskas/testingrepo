{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Databricks notebook source\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG System with MLflow Experiment Tracking\n",
        "\n",
        "This notebook implements a complete RAG (Retrieval-Augmented Generation) system for complaint summarization with:\n",
        "- Multiple LLM endpoint support\n",
        "- LLM-as-a-judge evaluation framework\n",
        "- MLflow experiment tracking and comparison\n",
        "- Comprehensive visualization and analysis\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Configuration\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import requests\n",
        "import re\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "from collections import defaultdict\n",
        "import mlflow\n",
        "import mlflow.pyfunc\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from mlflow.tracking import MlflowClient\n",
        "from math import pi\n",
        "\n",
        "# Set plot style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "# Configuration\n",
        "DATABRICKS_TOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
        "EMBEDDING_ENDPOINT = \"gte-endpoint\"\n",
        "WORKSPACE_URL = \"https://adb-7941446833400015.15.azuredatabricks.net\"\n",
        "\n",
        "# Available LLM endpoints for RAG\n",
        "AVAILABLE_LLMS = {\n",
        "    \"claude-sonnet-4.5\": \"databricks-claude-sonnet-4-5\",\n",
        "    \"claude-opus-4.5\": \"databricks-claude-opus-4-5\",\n",
        "    \"gpt-oss-120b\": \"databricks-gpt-oss-120b\"\n",
        "}\n",
        "\n",
        "# Judge LLMs for evaluation - all available models will serve as judges\n",
        "JUDGE_LLM_ENDPOINTS = {\n",
        "    \"judge-claude-sonnet-4.5\": \"databricks-claude-sonnet-4-5\",\n",
        "    \"judge-claude-opus-4.5\": \"databricks-claude-opus-4-5\",\n",
        "    \"judge-gpt-oss-120b\": \"databricks-gpt-oss-120b\"\n",
        "}\n",
        "\n",
        "# Vector Search Setup\n",
        "VECTOR_SEARCH_ENDPOINT = \"complaint-vector-endpoint\"\n",
        "VECTOR_SEARCH_INDEX = \"cntrl-busops-dev.complaints-1kh-gld.complaints_chunk_index\"\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Loading and Preprocessing\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "# Load data from PySpark table\n",
        "df_spark = spark.sql(\"SELECT * FROM `cntrl-busops-dev`.`complaints-1kh-gld`.`vw_complaint`\")\n",
        "\n",
        "# Convert to Pandas for processing\n",
        "df = df_spark.toPandas()\n",
        "\n",
        "print(f\"Loaded {len(df)} complaints\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "def extract_reference_number(text: str) -> str:\n",
        "    \"\"\"Extract reference_number from the all_columns text field.\"\"\"\n",
        "    if pd.isna(text) or not text:\n",
        "        return \"unknown\"\n",
        "    \n",
        "    # Try to extract reference_number from the beginning of the text\n",
        "    match = re.search(r'reference_number:\\s*([^\\|]+)', text, re.IGNORECASE)\n",
        "    if match:\n",
        "        return match.group(1).strip()\n",
        "    \n",
        "    # Fallback: try common complaint ID patterns\n",
        "    id_match = re.search(r'(CDCR\\w[|C-\\w]{5,})', text)\n",
        "    if id_match:\n",
        "        return id_match.group(1)\n",
        "    \n",
        "    return \"unknown\"\n",
        "\n",
        "\n",
        "# Extract complaint IDs from all_columns\n",
        "df['reference_number'] = df['all_columns'].apply(extract_reference_number)\n",
        "\n",
        "# Get all unique complaint IDs\n",
        "all_ids = df['reference_number'].unique().tolist()\n",
        "print(f\"Found {len(all_ids)} unique complaints\")\n",
        "\n",
        "# Build documents (each row's all_columns is the full document)\n",
        "docs = df['all_columns'].fillna(\"(no content)\").tolist()\n",
        "print(f\"Built {len(docs)} documents\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Document Chunking\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "def _chunk_text_with_overlap(text: str, chunk_size: int, overlap: int) -> List[str]:\n",
        "    \"\"\"Split text into overlapping chunks, breaking on sentence boundaries where possible.\"\"\"\n",
        "    if len(text) <= chunk_size:\n",
        "        return [text]\n",
        "    \n",
        "    chunks = []\n",
        "    start = 0\n",
        "    \n",
        "    while start < len(text):\n",
        "        end = start + chunk_size\n",
        "        \n",
        "        if end < len(text):\n",
        "            search_start = end - int(chunk_size * 0.2)\n",
        "            search_region = text[search_start:end]\n",
        "            last_period = search_region.rfind('. ')\n",
        "            last_newline = search_region.rfind('\\n')\n",
        "            last_pipe = search_region.rfind(' | ')\n",
        "            last_break = max(last_period, last_newline, last_pipe)\n",
        "            \n",
        "            if last_break != -1:\n",
        "                end = search_start + last_break + 1\n",
        "        \n",
        "        chunks.append(text[start:end].strip())\n",
        "        start = end - overlap\n",
        "    \n",
        "    return chunks\n",
        "\n",
        "\n",
        "def smart_chunk_document(\n",
        "    doc: str,\n",
        "    chunk_size: int = 512,\n",
        "    chunk_overlap: int = 128,\n",
        "    preserve_sections: bool = True\n",
        ") -> List[Tuple[str, Dict[str, str]]]:\n",
        "    \"\"\"Smart chunking with section preservation.\"\"\"\n",
        "    \n",
        "    chunks = []\n",
        "    reference_number = extract_reference_number(doc)\n",
        "    \n",
        "    if preserve_sections:\n",
        "        sections = re.split(r'(^##\\s+\\$)', doc, flags=re.MULTILINE)\n",
        "        current_section = \"\"\n",
        "        section_name = \"Main\"\n",
        "        \n",
        "        for i, part in enumerate(sections):\n",
        "            if part.strip().startswith(\"## \"):\n",
        "                section_name = part.strip().replace(\"## \", \"\")\n",
        "                current_section = part + \"\\n\"\n",
        "            elif part.strip():\n",
        "                current_section += part\n",
        "                \n",
        "                if len(current_section) >= chunk_size:\n",
        "                    section_chunks = _chunk_text_with_overlap(current_section, chunk_size, chunk_overlap)\n",
        "                    \n",
        "                    for j, chunk_text in enumerate(section_chunks):\n",
        "                        metadata = {\n",
        "                            \"reference_number\": reference_number,\n",
        "                            \"section\": section_name,\n",
        "                            \"chunk_index\": j,\n",
        "                            \"total_chunks\": len(section_chunks)\n",
        "                        }\n",
        "                        chunks.append((chunk_text, metadata))\n",
        "                    current_section = \"\"\n",
        "        \n",
        "        if current_section.strip():\n",
        "            metadata = {\n",
        "                \"reference_number\": reference_number,\n",
        "                \"section\": section_name,\n",
        "                \"chunk_index\": 0,\n",
        "                \"total_chunks\": 1\n",
        "            }\n",
        "            chunks.append((current_section, metadata))\n",
        "    else:\n",
        "        chunk_texts_list = _chunk_text_with_overlap(doc, chunk_size, chunk_overlap)\n",
        "        for i, chunk_text in enumerate(chunk_texts_list):\n",
        "            metadata = {\n",
        "                \"reference_number\": reference_number,\n",
        "                \"section\": \"Full-doc\",\n",
        "                \"chunk_index\": i,\n",
        "                \"total_chunks\": len(chunk_texts_list)\n",
        "            }\n",
        "            chunks.append((chunk_text, metadata))\n",
        "    \n",
        "    if not chunks and doc.strip():\n",
        "        metadata = {\n",
        "            \"reference_number\": reference_number,\n",
        "            \"section\": \"Main\",\n",
        "            \"chunk_index\": 0,\n",
        "            \"total_chunks\": 1\n",
        "        }\n",
        "        chunks.append((doc, metadata))\n",
        "    \n",
        "    return chunks\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "# Build chunks from all documents\n",
        "chunk_texts = []\n",
        "chunk_metadata = []\n",
        "\n",
        "for i, doc in enumerate(docs):\n",
        "    chunks = smart_chunk_document(doc, chunk_size=512, chunk_overlap=128, preserve_sections=True)\n",
        "    for chunk_text, metadata in chunks:\n",
        "        chunk_texts.append(chunk_text)\n",
        "        chunk_metadata.append(metadata)\n",
        "\n",
        "print(f\"Created {len(chunk_texts)} chunks from {len(docs)} complaints\")\n",
        "print(f\"Average chunks per complaint: {len(chunk_texts) / len(docs):.1f}\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Embedding Generation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "def _invocations_url(workspace_url: str, endpoint_name: str) -> str:\n",
        "    return f\"{workspace_url.rstrip('/')}/serving-endpoints/{endpoint_name}/invocations\"\n",
        "\n",
        "\n",
        "def _make_headers(token: str) -> dict:\n",
        "    return {\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"}\n",
        "\n",
        "\n",
        "def _parse_embeddings(resp_json):\n",
        "    \"\"\"Handle common embedding response shapes.\"\"\"\n",
        "    if isinstance(resp_json, dict):\n",
        "        if \"embeddings\" in resp_json and isinstance(resp_json[\"embeddings\"], list):\n",
        "            return resp_json[\"embeddings\"]\n",
        "        if \"data\" in resp_json:\n",
        "            data = resp_json[\"data\"]\n",
        "            if isinstance(data, list) and data and isinstance(data[0], dict) and \"embedding\" in data[0]:\n",
        "                return [row[\"embedding\"] for row in data]\n",
        "            if isinstance(data, dict) and \"embeddings\" in data:\n",
        "                return data[\"embeddings\"]\n",
        "    raise ValueError(f\"Unrecognized embeddings response shape: {list(resp_json.keys()) if isinstance(resp_json, dict) else type(resp_json)}\")\n",
        "\n",
        "\n",
        "def embed_databricks(\n",
        "    texts: List[str],\n",
        "    workspace_url: str = WORKSPACE_URL,\n",
        "    endpoint_name: str = EMBEDDING_ENDPOINT,\n",
        "    token: str = DATABRICKS_TOKEN,\n",
        "    batch_size: int = 64,\n",
        ") -> np.ndarray:\n",
        "    \"\"\"Returns L2-normalized embeddings (float32) shaped [N, D].\"\"\"\n",
        "    url = _invocations_url(workspace_url, endpoint_name)\n",
        "    headers = _make_headers(token)\n",
        "    \n",
        "    all_vecs: List[List[float]] = []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i : i + batch_size]\n",
        "        payload = {\"input\": batch}\n",
        "        response = requests.post(url, headers=headers, json=payload, timeout=60)\n",
        "        if response.ok:\n",
        "            vecs = _parse_embeddings(response.json())\n",
        "            all_vecs.extend(vecs)\n",
        "        else:\n",
        "            raise RuntimeError(f\"Embedding call failed: {response.status_code} {response.text[:500]}\")\n",
        "    \n",
        "    arr = np.array(all_vecs, dtype=\"float32\")\n",
        "    norms = np.linalg.norm(arr, axis=1, keepdims=True)\n",
        "    norms = np.clip(norms, 1e-12, None)\n",
        "    arr = arr / norms\n",
        "    return arr\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "print(\"Embedding all chunks...\")\n",
        "embeddings = embed_databricks(chunk_texts).astype(\"float32\")\n",
        "print(f\"Embedded {len(chunk_texts)} chunks, dimension: {embeddings.shape[1]}\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Vector Search and Retrieval\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "def retrieve_similar_chunks(\n",
        "    reference_number: str,\n",
        "    query_text: Optional[str] = None,\n",
        "    k: int = 10,\n",
        "    return_full_complaints: bool = False,\n",
        "    exclude_duplicates: bool = True\n",
        ") -> List[Tuple[str, float, str, Dict]]:\n",
        "    \"\"\"Retrieve similar chunks using Mosaic AI Vector Search REST API.\"\"\"\n",
        "    \n",
        "    # Build query vector\n",
        "    if query_text is not None:\n",
        "        q_vec = embed_databricks([query_text])[0].tolist()\n",
        "    else:\n",
        "        complaint_chunk_indices = []\n",
        "        for i, meta in enumerate(chunk_metadata):\n",
        "            if meta['reference_number'] == reference_number:\n",
        "                complaint_chunk_indices.append(i)\n",
        "        \n",
        "        if not complaint_chunk_indices:\n",
        "            return []\n",
        "        \n",
        "        q_idx = complaint_chunk_indices[0]\n",
        "        q_vec = embeddings[q_idx].tolist()\n",
        "    \n",
        "    # Call Vector Search REST API\n",
        "    search_k = k * 5 if exclude_duplicates else k * 2\n",
        "    \n",
        "    url = f\"{WORKSPACE_URL}/api/2.0/vector-search/indexes/{VECTOR_SEARCH_INDEX}/query\"\n",
        "    \n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {DATABRICKS_TOKEN}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    \n",
        "    payload = {\n",
        "        \"query_vector\": q_vec,\n",
        "        \"num_results\": search_k,\n",
        "        \"columns\": [\"chunk_id\", \"reference_number\", \"section\", \"chunk_index\", \"chunk_text\"]\n",
        "    }\n",
        "    \n",
        "    response = requests.post(url, headers=headers, json=payload, timeout=120)\n",
        "    \n",
        "    if not response.ok:\n",
        "        raise RuntimeError(f\"Vector Search failed: {response.status_code} {response.text[:500]}\")\n",
        "    \n",
        "    results_raw = response.json()\n",
        "    \n",
        "    seen_complaints = set([reference_number])\n",
        "    results = []\n",
        "    duplicate_exclusion_set = set()\n",
        "    \n",
        "    for row in results_raw.get(\"result\", {}).get(\"data_array\", []):\n",
        "        chunk_id, chunk_cid, section, chunk_idx, text, score = row\n",
        "        \n",
        "        if exclude_duplicates and int(chunk_id) in duplicate_exclusion_set:\n",
        "            continue\n",
        "        \n",
        "        if chunk_cid == reference_number:\n",
        "            continue\n",
        "        \n",
        "        meta = {\n",
        "            \"reference_number\": chunk_cid,\n",
        "            \"section\": section,\n",
        "            \"chunk_index\": chunk_idx\n",
        "        }\n",
        "        \n",
        "        if return_full_complaints:\n",
        "            if chunk_cid not in seen_complaints:\n",
        "                seen_complaints.add(chunk_cid)\n",
        "                try:\n",
        "                    doc_idx = all_ids.index(chunk_cid)\n",
        "                    full_doc = docs[doc_idx]\n",
        "                except ValueError:\n",
        "                    full_doc = text\n",
        "                results.append((chunk_cid, float(score), full_doc, meta))\n",
        "        else:\n",
        "            results.append((chunk_cid, float(score), text, meta))\n",
        "        \n",
        "        if len(results) >= k:\n",
        "            break\n",
        "    \n",
        "    return results\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. LLM Integration\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "def call_llm_endpoint(\n",
        "    prompt: str, \n",
        "    endpoint_name: str,\n",
        "    max_new_tokens: int = 320, \n",
        "    temperature: float = 0.0\n",
        ") -> str:\n",
        "    \"\"\"Sends a chat-style prompt to a Databricks-hosted LLM endpoint.\"\"\"\n",
        "    \n",
        "    url = f\"{WORKSPACE_URL}/serving-endpoints/{endpoint_name}/invocations\"\n",
        "    \n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {DATABRICKS_TOKEN}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    \n",
        "    payload = {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"You are an expert complaints analyst.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        \"max_tokens\": max_new_tokens,\n",
        "        \"temperature\": temperature\n",
        "    }\n",
        "    \n",
        "    response = requests.post(url, headers=headers, json=payload, timeout=120)\n",
        "    \n",
        "    if response.ok:\n",
        "        result = response.json()\n",
        "        return result[\"choices\"][0][\"message\"][\"content\"]\n",
        "    else:\n",
        "        raise RuntimeError(f\"LLM call failed: {response.status_code} {response.text[:500]}\")\n",
        "\n",
        "\n",
        "SUMMARY_PROMPT = \"\"\"You are an expert complaints analyst writing a clear, factual and chronological summary of a customer complaint.\n",
        "\n",
        "Use ONLY the provided context - do not add information that isn't present.\n",
        "\n",
        "Your goal is to create a concise narrative (5-10 sentences) that describes the complaint journey in order of events.\n",
        "Include:\n",
        "- When and how the complaint was received\n",
        "- What the customer raised or alleged\n",
        "- How the bank investigated and communicated during the process\n",
        "- Key findings, decisions, or redress outcomes\n",
        "- Any SLA breaches, delays, or escalation to FOS if applicable\n",
        "- When and how the complaint was closed\n",
        "\n",
        "Write it in professional plain English, past tense, and in chronological order (oldest events first).\n",
        "Avoid bullet points or headings - return one coherent paragraph.\n",
        "\n",
        "---\n",
        "TARGET COMPLAINT CONTEXT:\n",
        "{target}\n",
        "\n",
        "---\n",
        "SIMILAR CASES (for style reference only, do not copy facts):\n",
        "{neighbors}\n",
        "\n",
        "Now write the chronological summary of the target complaint only.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def rag_summarize(\n",
        "    reference_number: str,\n",
        "    llm_endpoint: str,\n",
        "    k_neighbors: int = 4,\n",
        "    exclude_duplicates: bool = True,\n",
        "    temperature: float = 0.6,\n",
        "    max_tokens: int = 5000\n",
        ") -> str:\n",
        "    \"\"\"Generate a RAG-enhanced summary using specified LLM endpoint.\"\"\"\n",
        "    \n",
        "    if reference_number not in all_ids:\n",
        "        return f\"Complaint {reference_number} not found.\"\n",
        "    \n",
        "    target_doc = docs[all_ids.index(reference_number)]\n",
        "    \n",
        "    similar = retrieve_similar_chunks(\n",
        "        reference_number,\n",
        "        k=k_neighbors,\n",
        "        return_full_complaints=True,\n",
        "        exclude_duplicates=exclude_duplicates\n",
        "    )\n",
        "    \n",
        "    neigh_text = \"\\n\\n---\\n\\n\".join([\n",
        "        f\"# Similar {i} ({cid}) (score: {score:.3f})\\n{doc[:6000]}\"\n",
        "        for i, (cid, score, doc, meta) in enumerate(similar)\n",
        "    ])[:12000]\n",
        "    \n",
        "    prompt = SUMMARY_PROMPT.format(\n",
        "        target=target_doc[:24000],\n",
        "        neighbors=neigh_text\n",
        "    )\n",
        "    \n",
        "    return call_llm_endpoint(prompt, endpoint_name=llm_endpoint, max_new_tokens=max_tokens, temperature=temperature)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. LLM-as-a-Judge Evaluation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "JUDGE_PROMPT = \"\"\"You are an expert evaluator assessing the quality of complaint summaries.\n",
        "\n",
        "You will be given:\n",
        "1. The original complaint context\n",
        "2. A generated summary\n",
        "\n",
        "Evaluate the summary on the following criteria (rate each 1-5, where 5 is best):\n",
        "\n",
        "**ACCURACY**: Does the summary contain only factual information from the original complaint? Are there any hallucinations or invented details?\n",
        "- 5: Completely accurate, no hallucinations\n",
        "- 3: Mostly accurate with minor discrepancies\n",
        "- 1: Contains significant inaccuracies or hallucinations\n",
        "\n",
        "**COMPLETENESS**: Does the summary capture all key events, decisions, and outcomes from the complaint?\n",
        "- 5: All critical information included\n",
        "- 3: Some important details missing\n",
        "- 1: Major gaps in coverage\n",
        "\n",
        "**CHRONOLOGY**: Are events presented in the correct temporal order?\n",
        "- 5: Perfect chronological flow\n",
        "- 3: Mostly chronological with minor issues\n",
        "- 1: Confusing or incorrect ordering\n",
        "\n",
        "**CLARITY**: Is the summary well-written, concise, and easy to understand?\n",
        "- 5: Exceptionally clear and professional\n",
        "- 3: Adequate but could be clearer\n",
        "- 1: Confusing or poorly written\n",
        "\n",
        "**CONCISENESS**: Is the summary appropriately brief without unnecessary detail?\n",
        "- 5: Perfect balance of detail and brevity\n",
        "- 3: Somewhat verbose or too terse\n",
        "- 1: Far too long or missing critical context\n",
        "\n",
        "---\n",
        "ORIGINAL COMPLAINT:\n",
        "{original_context}\n",
        "\n",
        "---\n",
        "GENERATED SUMMARY:\n",
        "{summary}\n",
        "\n",
        "---\n",
        "Provide your evaluation in the following JSON format:\n",
        "{{\n",
        "    \"accuracy\": <score 1-5>,\n",
        "    \"accuracy_reasoning\": \"<brief explanation>\",\n",
        "    \"completeness\": <score 1-5>,\n",
        "    \"completeness_reasoning\": \"<brief explanation>\",\n",
        "    \"chronology\": <score 1-5>,\n",
        "    \"chronology_reasoning\": \"<brief explanation>\",\n",
        "    \"clarity\": <score 1-5>,\n",
        "    \"clarity_reasoning\": \"<brief explanation>\",\n",
        "    \"conciseness\": <score 1-5>,\n",
        "    \"conciseness_reasoning\": \"<brief explanation>\",\n",
        "    \"overall_score\": <average of all scores>,\n",
        "    \"overall_assessment\": \"<2-3 sentence summary of strengths and weaknesses>\"\n",
        "}}\n",
        "\n",
        "Return ONLY the JSON, no other text.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def judge_summary(\n",
        "    reference_number: str,\n",
        "    summary: str,\n",
        "    judge_endpoints: Dict[str, str] = None\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Use multiple LLMs as judges to evaluate a generated summary and average the results.\n",
        "    \n",
        "    Args:\n",
        "        reference_number: The complaint ID\n",
        "        summary: The generated summary to evaluate\n",
        "        judge_endpoints: Dictionary of judge names to endpoint names (uses all available by default)\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary containing averaged evaluation scores, individual judge results, and reasoning\n",
        "    \"\"\"\n",
        "    \n",
        "    if reference_number not in all_ids:\n",
        "        raise ValueError(f\"Complaint {reference_number} not found\")\n",
        "    \n",
        "    if judge_endpoints is None:\n",
        "        judge_endpoints = JUDGE_LLM_ENDPOINTS\n",
        "    \n",
        "    original_context = docs[all_ids.index(reference_number)]\n",
        "    \n",
        "    prompt = JUDGE_PROMPT.format(\n",
        "        original_context=original_context[:20000],\n",
        "        summary=summary\n",
        "    )\n",
        "    \n",
        "    # Collect evaluations from all judges\n",
        "    all_judge_results = {}\n",
        "    \n",
        "    for judge_name, judge_endpoint in judge_endpoints.items():\n",
        "        print(f\"  Evaluating with {judge_name}...\")\n",
        "        \n",
        "        try:\n",
        "            response = call_llm_endpoint(\n",
        "                prompt, \n",
        "                endpoint_name=judge_endpoint,\n",
        "                max_new_tokens=2000,\n",
        "                temperature=0.1\n",
        "            )\n",
        "            \n",
        "            # Parse JSON response\n",
        "            response = response.strip()\n",
        "            if response.startswith(\"```json\"):\n",
        "                response = response[7:]\n",
        "            if response.startswith(\"```\"):\n",
        "                response = response[3:]\n",
        "            if response.endswith(\"```\"):\n",
        "                response = response[:-3]\n",
        "            response = response.strip()\n",
        "            \n",
        "            eval_result = json.loads(response)\n",
        "            all_judge_results[judge_name] = eval_result\n",
        "            \n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"  Warning: Failed to parse {judge_name} response: {e}\")\n",
        "            # Use neutral scores if parsing fails\n",
        "            all_judge_results[judge_name] = {\n",
        "                \"accuracy\": 3,\n",
        "                \"completeness\": 3,\n",
        "                \"chronology\": 3,\n",
        "                \"clarity\": 3,\n",
        "                \"conciseness\": 3,\n",
        "                \"overall_score\": 3,\n",
        "                \"overall_assessment\": f\"Failed to parse {judge_name} evaluation\",\n",
        "                \"parse_error\": True\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"  Warning: Error with {judge_name}: {e}\")\n",
        "            all_judge_results[judge_name] = {\n",
        "                \"accuracy\": 3,\n",
        "                \"completeness\": 3,\n",
        "                \"chronology\": 3,\n",
        "                \"clarity\": 3,\n",
        "                \"conciseness\": 3,\n",
        "                \"overall_score\": 3,\n",
        "                \"overall_assessment\": f\"Error with {judge_name}: {str(e)}\",\n",
        "                \"evaluation_error\": True\n",
        "            }\n",
        "    \n",
        "    # Calculate averaged scores across all judges\n",
        "    metrics = [\"accuracy\", \"completeness\", \"chronology\", \"clarity\", \"conciseness\", \"overall_score\"]\n",
        "    averaged_scores = {}\n",
        "    \n",
        "    for metric in metrics:\n",
        "        scores = [result[metric] for result in all_judge_results.values() if metric in result]\n",
        "        if scores:\n",
        "            averaged_scores[f\"avg_{metric}\"] = np.mean(scores)\n",
        "            averaged_scores[f\"std_{metric}\"] = np.std(scores)\n",
        "            averaged_scores[f\"min_{metric}\"] = np.min(scores)\n",
        "            averaged_scores[f\"max_{metric}\"] = np.max(scores)\n",
        "        else:\n",
        "            averaged_scores[f\"avg_{metric}\"] = 3.0\n",
        "            averaged_scores[f\"std_{metric}\"] = 0.0\n",
        "            averaged_scores[f\"min_{metric}\"] = 3.0\n",
        "            averaged_scores[f\"max_{metric}\"] = 3.0\n",
        "    \n",
        "    # Compile reasoning from all judges\n",
        "    reasoning_summary = {}\n",
        "    for metric in [\"accuracy\", \"completeness\", \"chronology\", \"clarity\", \"conciseness\"]:\n",
        "        reasoning_key = f\"{metric}_reasoning\"\n",
        "        reasoning_summary[reasoning_key] = {\n",
        "            judge: result.get(reasoning_key, \"N/A\") \n",
        "            for judge, result in all_judge_results.items()\n",
        "        }\n",
        "    \n",
        "    # Compile overall assessments\n",
        "    overall_assessments = {\n",
        "        judge: result.get(\"overall_assessment\", \"N/A\")\n",
        "        for judge, result in all_judge_results.items()\n",
        "    }\n",
        "    \n",
        "    # Return comprehensive results\n",
        "    return {\n",
        "        # Averaged scores (main metrics to use)\n",
        "        \"accuracy\": averaged_scores[\"avg_accuracy\"],\n",
        "        \"completeness\": averaged_scores[\"avg_completeness\"],\n",
        "        \"chronology\": averaged_scores[\"avg_chronology\"],\n",
        "        \"clarity\": averaged_scores[\"avg_clarity\"],\n",
        "        \"conciseness\": averaged_scores[\"avg_conciseness\"],\n",
        "        \"overall_score\": averaged_scores[\"avg_overall_score\"],\n",
        "        \n",
        "        # Score statistics\n",
        "        \"accuracy_std\": averaged_scores[\"std_accuracy\"],\n",
        "        \"completeness_std\": averaged_scores[\"std_completeness\"],\n",
        "        \"chronology_std\": averaged_scores[\"std_chronology\"],\n",
        "        \"clarity_std\": averaged_scores[\"std_clarity\"],\n",
        "        \"conciseness_std\": averaged_scores[\"std_conciseness\"],\n",
        "        \"overall_score_std\": averaged_scores[\"std_overall_score\"],\n",
        "        \n",
        "        \"accuracy_range\": (averaged_scores[\"min_accuracy\"], averaged_scores[\"max_accuracy\"]),\n",
        "        \"completeness_range\": (averaged_scores[\"min_completeness\"], averaged_scores[\"max_completeness\"]),\n",
        "        \"chronology_range\": (averaged_scores[\"min_chronology\"], averaged_scores[\"max_chronology\"]),\n",
        "        \"clarity_range\": (averaged_scores[\"min_clarity\"], averaged_scores[\"max_clarity\"]),\n",
        "        \"conciseness_range\": (averaged_scores[\"min_conciseness\"], averaged_scores[\"max_conciseness\"]),\n",
        "        \"overall_score_range\": (averaged_scores[\"min_overall_score\"], averaged_scores[\"max_overall_score\"]),\n",
        "        \n",
        "        # Individual judge results\n",
        "        \"individual_judge_scores\": all_judge_results,\n",
        "        \"num_judges\": len(all_judge_results),\n",
        "        \n",
        "        # Reasoning from all judges\n",
        "        \"reasoning_by_judge\": reasoning_summary,\n",
        "        \"overall_assessments\": overall_assessments,\n",
        "        \n",
        "        # Consensus metrics\n",
        "        \"judge_agreement\": 1 - (averaged_scores[\"std_overall_score\"] / 5.0) if averaged_scores[\"std_overall_score\"] < 5.0 else 0.0,  # 0-1 scale\n",
        "        \"overall_assessment\": f\"Average of {len(all_judge_results)} judges. Agreement: {(1 - averaged_scores['std_overall_score'] / 5.0):.2%}\"\n",
        "    }\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. MLflow Experiment Framework\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "def run_rag_experiment(\n",
        "    experiment_name: str = \"/Users/your_email@domain.com/rag_llm_comparison\",\n",
        "    test_complaints: List[str] = None,\n",
        "    llm_configs: Dict[str, Dict] = None,\n",
        "    k_neighbors: int = 4,\n",
        "    judge_endpoints: Dict[str, str] = None\n",
        "):\n",
        "    \"\"\"Run RAG experiments with different LLMs and track results in MLflow.\"\"\"\n",
        "    \n",
        "    mlflow.set_experiment(experiment_name)\n",
        "    \n",
        "    if test_complaints is None:\n",
        "        test_complaints = all_ids[:10] if len(all_ids) >= 10 else all_ids\n",
        "    \n",
        "    if llm_configs is None:\n",
        "        llm_configs = {\n",
        "            \"claude-sonnet-4.5-temp0.3\": {\n",
        "                \"endpoint\": AVAILABLE_LLMS[\"claude-sonnet-4.5\"],\n",
        "                \"temperature\": 0.3,\n",
        "                \"max_tokens\": 5000\n",
        "            },\n",
        "            \"claude-sonnet-4.5-temp0.6\": {\n",
        "                \"endpoint\": AVAILABLE_LLMS[\"claude-sonnet-4.5\"],\n",
        "                \"temperature\": 0.6,\n",
        "                \"max_tokens\": 5000\n",
        "            },\n",
        "            \"claude-opus-4.5\": {\n",
        "                \"endpoint\": AVAILABLE_LLMS[\"claude-opus-4.5\"],\n",
        "                \"temperature\": 0.6,\n",
        "                \"max_tokens\": 5000\n",
        "            },\n",
        "            \"gpt-oss-120b\": {\n",
        "                \"endpoint\": AVAILABLE_LLMS[\"gpt-oss-120b\"],\n",
        "                \"temperature\": 0.6,\n",
        "                \"max_tokens\": 5000\n",
        "            }\n",
        "        }\n",
        "    \n",
        "    if judge_endpoints is None:\n",
        "        judge_endpoints = JUDGE_LLM_ENDPOINTS\n",
        "    \n",
        "    results_summary = []\n",
        "    \n",
        "    for llm_name, config in llm_configs.items():\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"Running experiment: {llm_name}\")\n",
        "        print(f\"{'='*80}\\n\")\n",
        "        \n",
        "        with mlflow.start_run(run_name=f\"{llm_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"):\n",
        "            \n",
        "            mlflow.log_param(\"llm_name\", llm_name)\n",
        "            mlflow.log_param(\"llm_endpoint\", config[\"endpoint\"])\n",
        "            mlflow.log_param(\"temperature\", config[\"temperature\"])\n",
        "            mlflow.log_param(\"max_tokens\", config[\"max_tokens\"])\n",
        "            mlflow.log_param(\"k_neighbors\", k_neighbors)\n",
        "            mlflow.log_param(\"num_test_complaints\", len(test_complaints))\n",
        "            mlflow.log_param(\"num_judges\", len(judge_endpoints))\n",
        "            mlflow.log_param(\"judge_endpoints\", \", \".join(judge_endpoints.keys()))\n",
        "            \n",
        "            all_summaries = []\n",
        "            all_evaluations = []\n",
        "            \n",
        "            for i, complaint_id in enumerate(test_complaints):\n",
        "                print(f\"Processing complaint {i+1}/{len(test_complaints)}: {complaint_id}\")\n",
        "                \n",
        "                try:\n",
        "                    summary = rag_summarize(\n",
        "                        reference_number=complaint_id,\n",
        "                        llm_endpoint=config[\"endpoint\"],\n",
        "                        k_neighbors=k_neighbors,\n",
        "                        temperature=config[\"temperature\"],\n",
        "                        max_tokens=config[\"max_tokens\"]\n",
        "                    )\n",
        "                    \n",
        "                    evaluation = judge_summary(\n",
        "                        reference_number=complaint_id,\n",
        "                        summary=summary,\n",
        "                        judge_endpoints=judge_endpoints\n",
        "                    )\n",
        "                    \n",
        "                    all_summaries.append({\n",
        "                        \"complaint_id\": complaint_id,\n",
        "                        \"summary\": summary,\n",
        "                        \"summary_length\": len(summary)\n",
        "                    })\n",
        "                    \n",
        "                    all_evaluations.append({\n",
        "                        \"complaint_id\": complaint_id,\n",
        "                        **evaluation\n",
        "                    })\n",
        "                    \n",
        "                    # Log individual complaint metrics (averaged scores)\n",
        "                    mlflow.log_metric(f\"accuracy_{i}\", evaluation[\"accuracy\"])\n",
        "                    mlflow.log_metric(f\"completeness_{i}\", evaluation[\"completeness\"])\n",
        "                    mlflow.log_metric(f\"chronology_{i}\", evaluation[\"chronology\"])\n",
        "                    mlflow.log_metric(f\"clarity_{i}\", evaluation[\"clarity\"])\n",
        "                    mlflow.log_metric(f\"conciseness_{i}\", evaluation[\"conciseness\"])\n",
        "                    mlflow.log_metric(f\"overall_score_{i}\", evaluation[\"overall_score\"])\n",
        "                    \n",
        "                    # Log standard deviations for each complaint (judge agreement)\n",
        "                    mlflow.log_metric(f\"accuracy_std_{i}\", evaluation[\"accuracy_std\"])\n",
        "                    mlflow.log_metric(f\"overall_score_std_{i}\", evaluation[\"overall_score_std\"])\n",
        "                    mlflow.log_metric(f\"judge_agreement_{i}\", evaluation[\"judge_agreement\"])\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {complaint_id}: {e}\")\n",
        "                    continue\n",
        "            \n",
        "            if all_evaluations:\n",
        "                avg_accuracy = np.mean([e[\"accuracy\"] for e in all_evaluations])\n",
        "                avg_completeness = np.mean([e[\"completeness\"] for e in all_evaluations])\n",
        "                avg_chronology = np.mean([e[\"chronology\"] for e in all_evaluations])\n",
        "                avg_clarity = np.mean([e[\"clarity\"] for e in all_evaluations])\n",
        "                avg_conciseness = np.mean([e[\"conciseness\"] for e in all_evaluations])\n",
        "                avg_overall = np.mean([e[\"overall_score\"] for e in all_evaluations])\n",
        "                avg_summary_length = np.mean([s[\"summary_length\"] for s in all_summaries])\n",
        "                \n",
        "                # Average judge agreement across all complaints\n",
        "                avg_judge_agreement = np.mean([e[\"judge_agreement\"] for e in all_evaluations])\n",
        "                \n",
        "                mlflow.log_metric(\"avg_accuracy\", avg_accuracy)\n",
        "                mlflow.log_metric(\"avg_completeness\", avg_completeness)\n",
        "                mlflow.log_metric(\"avg_chronology\", avg_chronology)\n",
        "                mlflow.log_metric(\"avg_clarity\", avg_clarity)\n",
        "                mlflow.log_metric(\"avg_conciseness\", avg_conciseness)\n",
        "                mlflow.log_metric(\"avg_overall_score\", avg_overall)\n",
        "                mlflow.log_metric(\"avg_summary_length\", avg_summary_length)\n",
        "                mlflow.log_metric(\"avg_judge_agreement\", avg_judge_agreement)\n",
        "                \n",
        "                # Log standard deviations (variability across complaints)\n",
        "                mlflow.log_metric(\"std_accuracy\", np.std([e[\"accuracy\"] for e in all_evaluations]))\n",
        "                mlflow.log_metric(\"std_overall_score\", np.std([e[\"overall_score\"] for e in all_evaluations]))\n",
        "                \n",
        "                # Log average within-complaint standard deviations (judge disagreement)\n",
        "                mlflow.log_metric(\"avg_accuracy_std\", np.mean([e[\"accuracy_std\"] for e in all_evaluations]))\n",
        "                mlflow.log_metric(\"avg_overall_score_std\", np.mean([e[\"overall_score_std\"] for e in all_evaluations]))\n",
        "                \n",
        "                summaries_df = pd.DataFrame(all_summaries)\n",
        "                evaluations_df = pd.DataFrame(all_evaluations)\n",
        "                \n",
        "                # Create a detailed judge breakdown file\n",
        "                judge_details = []\n",
        "                for eval_data in all_evaluations:\n",
        "                    complaint_id = eval_data[\"complaint_id\"]\n",
        "                    for judge_name, judge_result in eval_data[\"individual_judge_scores\"].items():\n",
        "                        judge_details.append({\n",
        "                            \"complaint_id\": complaint_id,\n",
        "                            \"judge\": judge_name,\n",
        "                            \"accuracy\": judge_result.get(\"accuracy\", None),\n",
        "                            \"completeness\": judge_result.get(\"completeness\", None),\n",
        "                            \"chronology\": judge_result.get(\"chronology\", None),\n",
        "                            \"clarity\": judge_result.get(\"clarity\", None),\n",
        "                            \"conciseness\": judge_result.get(\"conciseness\", None),\n",
        "                            \"overall_score\": judge_result.get(\"overall_score\", None),\n",
        "                            \"overall_assessment\": judge_result.get(\"overall_assessment\", \"\")\n",
        "                        })\n",
        "                \n",
        "                judge_breakdown_df = pd.DataFrame(judge_details)\n",
        "                \n",
        "                summaries_df.to_csv(\"summaries.csv\", index=False)\n",
        "                evaluations_df.to_csv(\"evaluations.csv\", index=False)\n",
        "                judge_breakdown_df.to_csv(\"judge_breakdown.csv\", index=False)\n",
        "                \n",
        "                mlflow.log_artifact(\"summaries.csv\")\n",
        "                mlflow.log_artifact(\"evaluations.csv\")\n",
        "                mlflow.log_artifact(\"judge_breakdown.csv\")\n",
        "                \n",
        "                os.remove(\"summaries.csv\")\n",
        "                os.remove(\"evaluations.csv\")\n",
        "                os.remove(\"judge_breakdown.csv\")\n",
        "                \n",
        "                results_summary.append({\n",
        "                    \"llm_name\": llm_name,\n",
        "                    \"avg_accuracy\": avg_accuracy,\n",
        "                    \"avg_completeness\": avg_completeness,\n",
        "                    \"avg_chronology\": avg_chronology,\n",
        "                    \"avg_clarity\": avg_clarity,\n",
        "                    \"avg_conciseness\": avg_conciseness,\n",
        "                    \"avg_overall_score\": avg_overall,\n",
        "                    \"avg_summary_length\": avg_summary_length,\n",
        "                    \"avg_judge_agreement\": avg_judge_agreement\n",
        "                })\n",
        "                \n",
        "                print(f\"\\n{llm_name} Results:\")\n",
        "                print(f\"  Avg Accuracy: {avg_accuracy:.2f}\")\n",
        "                print(f\"  Avg Completeness: {avg_completeness:.2f}\")\n",
        "                print(f\"  Avg Chronology: {avg_chronology:.2f}\")\n",
        "                print(f\"  Avg Clarity: {avg_clarity:.2f}\")\n",
        "                print(f\"  Avg Conciseness: {avg_conciseness:.2f}\")\n",
        "                print(f\"  Avg Overall Score: {avg_overall:.2f}\")\n",
        "                print(f\"  Avg Summary Length: {avg_summary_length:.0f} chars\")\n",
        "                print(f\"  Avg Judge Agreement: {avg_judge_agreement:.2%}\")\n",
        "    \n",
        "    comparison_df = pd.DataFrame(results_summary)\n",
        "    comparison_df = comparison_df.sort_values(\"avg_overall_score\", ascending=False)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"EXPERIMENT SUMMARY - RANKED BY OVERALL SCORE\")\n",
        "    print(\"=\"*80)\n",
        "    print(comparison_df.to_string(index=False))\n",
        "    \n",
        "    return comparison_df\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Run Experiment\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "# Select test complaints\n",
        "test_complaint_ids = [\n",
        "    \"CDCR-23502631\", \n",
        "    \"CDCR-21029850\", \n",
        "    \"CDCR-33381031\", \n",
        "    \"CDCR-35951561\"\n",
        "]\n",
        "\n",
        "# Define LLM configurations to test\n",
        "llm_configs_to_test = {\n",
        "    \"claude-sonnet-4.5-balanced\": {\n",
        "        \"endpoint\": AVAILABLE_LLMS[\"claude-sonnet-4.5\"],\n",
        "        \"temperature\": 0.6,\n",
        "        \"max_tokens\": 5000\n",
        "    },\n",
        "    \"claude-sonnet-4.5-conservative\": {\n",
        "        \"endpoint\": AVAILABLE_LLMS[\"claude-sonnet-4.5\"],\n",
        "        \"temperature\": 0.3,\n",
        "        \"max_tokens\": 5000\n",
        "    },\n",
        "    \"claude-opus-4.5\": {\n",
        "        \"endpoint\": AVAILABLE_LLMS[\"claude-opus-4.5\"],\n",
        "        \"temperature\": 0.6,\n",
        "        \"max_tokens\": 5000\n",
        "    },\n",
        "    \"gpt-oss-120b\": {\n",
        "        \"endpoint\": AVAILABLE_LLMS[\"gpt-oss-120b\"],\n",
        "        \"temperature\": 0.6,\n",
        "        \"max_tokens\": 5000\n",
        "    }\n",
        "}\n",
        "\n",
        "# Run the experiment\n",
        "results = run_rag_experiment(\n",
        "    experiment_name=\"/Users/your_email@domain.com/rag_llm_comparison\",\n",
        "    test_complaints=test_complaint_ids,\n",
        "    llm_configs=llm_configs_to_test,\n",
        "    k_neighbors=4\n",
        ")\n",
        "\n",
        "print(\"\\nExperiment complete! Check MLflow UI for detailed results.\")\n",
        "print(f\"Best performing model: {results.iloc[0]['llm_name']}\")\n",
        "print(f\"Overall score: {results.iloc[0]['avg_overall_score']:.2f}/5.0\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Load and Analyze Experiment Results\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "# Initialize MLflow client\n",
        "client = MlflowClient()\n",
        "\n",
        "# Get the experiment\n",
        "experiment_name = \"/Users/your_email@domain.com/rag_llm_comparison\"\n",
        "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
        "\n",
        "if experiment is None:\n",
        "    print(f\"Experiment '{experiment_name}' not found!\")\n",
        "else:\n",
        "    print(f\"Found experiment: {experiment.name}\")\n",
        "    print(f\"Experiment ID: {experiment.experiment_id}\")\n",
        "    \n",
        "    # Get all runs from the experiment\n",
        "    runs = mlflow.search_runs(\n",
        "        experiment_ids=[experiment.experiment_id],\n",
        "        order_by=[\"start_time DESC\"]\n",
        "    )\n",
        "    \n",
        "    print(f\"Total runs: {len(runs)}\")\n",
        "    \n",
        "    # Display basic info about runs\n",
        "    display(runs[['run_id', 'start_time', 'status', 'params.llm_name', \n",
        "                  'metrics.avg_overall_score']].head(10))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Model Performance Comparison\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "# Extract key metrics for comparison\n",
        "comparison_metrics = runs[[\n",
        "    'params.llm_name',\n",
        "    'params.temperature',\n",
        "    'metrics.avg_accuracy',\n",
        "    'metrics.avg_completeness',\n",
        "    'metrics.avg_chronology',\n",
        "    'metrics.avg_clarity',\n",
        "    'metrics.avg_conciseness',\n",
        "    'metrics.avg_overall_score',\n",
        "    'metrics.avg_summary_length',\n",
        "    'metrics.std_overall_score'\n",
        "]].copy()\n",
        "\n",
        "comparison_metrics = comparison_metrics.sort_values('metrics.avg_overall_score', ascending=False)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"MODEL PERFORMANCE COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "display(comparison_metrics)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Visualization: Overall Scores by Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "models = comparison_metrics['params.llm_name'].values\n",
        "scores = comparison_metrics['metrics.avg_overall_score'].values\n",
        "std_devs = comparison_metrics['metrics.std_overall_score'].values\n",
        "\n",
        "bars = ax.barh(models, scores, xerr=std_devs, capsize=5, color='steelblue', alpha=0.8)\n",
        "\n",
        "for i, (bar, score) in enumerate(zip(bars, scores)):\n",
        "    ax.text(score + 0.1, i, f'{score:.2f}', va='center', fontweight='bold')\n",
        "\n",
        "ax.set_xlabel('Average Overall Score (1-5)', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('LLM Configuration', fontsize=12, fontweight='bold')\n",
        "ax.set_title('RAG Summarization: Overall Model Performance', fontsize=14, fontweight='bold')\n",
        "ax.set_xlim(0, 5.5)\n",
        "ax.grid(axis='x', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Multi-Dimensional Performance - Radar Chart\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "categories = ['Accuracy', 'Completeness', 'Chronology', 'Clarity', 'Conciseness']\n",
        "metric_cols = [\n",
        "    'metrics.avg_accuracy',\n",
        "    'metrics.avg_completeness', \n",
        "    'metrics.avg_chronology',\n",
        "    'metrics.avg_clarity',\n",
        "    'metrics.avg_conciseness'\n",
        "]\n",
        "\n",
        "top_models = comparison_metrics.head(4)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
        "\n",
        "num_vars = len(categories)\n",
        "angles = [n / float(num_vars) * 2 * pi for n in range(num_vars)]\n",
        "angles += angles[:1]\n",
        "\n",
        "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
        "for idx, (_, row) in enumerate(top_models.iterrows()):\n",
        "    values = [row[col] for col in metric_cols]\n",
        "    values += values[:1]\n",
        "    \n",
        "    ax.plot(angles, values, 'o-', linewidth=2, label=row['params.llm_name'], color=colors[idx])\n",
        "    ax.fill(angles, values, alpha=0.15, color=colors[idx])\n",
        "\n",
        "ax.set_theta_offset(pi / 2)\n",
        "ax.set_theta_direction(-1)\n",
        "\n",
        "ax.set_xticks(angles[:-1])\n",
        "ax.set_xticklabels(categories, size=11, fontweight='bold')\n",
        "\n",
        "ax.set_ylim(0, 5)\n",
        "ax.set_yticks([1, 2, 3, 4, 5])\n",
        "ax.set_yticklabels(['1', '2', '3', '4', '5'], size=9)\n",
        "\n",
        "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=10)\n",
        "\n",
        "ax.set_title('Multi-Dimensional Performance Comparison', \n",
        "             size=14, fontweight='bold', pad=20)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Detailed Metrics Distribution\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "fig.suptitle('Score Distributions Across Models', fontsize=16, fontweight='bold')\n",
        "\n",
        "metrics_to_plot = [\n",
        "    ('metrics.avg_accuracy', 'Accuracy'),\n",
        "    ('metrics.avg_completeness', 'Completeness'),\n",
        "    ('metrics.avg_chronology', 'Chronology'),\n",
        "    ('metrics.avg_clarity', 'Clarity'),\n",
        "    ('metrics.avg_conciseness', 'Conciseness'),\n",
        "    ('metrics.avg_overall_score', 'Overall Score')\n",
        "]\n",
        "\n",
        "for idx, (metric_col, title) in enumerate(metrics_to_plot):\n",
        "    ax = axes[idx // 3, idx % 3]\n",
        "    \n",
        "    plot_data = []\n",
        "    labels = []\n",
        "    for _, row in top_models.iterrows():\n",
        "        plot_data.append(row[metric_col])\n",
        "        labels.append(row['params.llm_name'])\n",
        "    \n",
        "    bars = ax.barh(range(len(labels)), plot_data, color='steelblue', alpha=0.7)\n",
        "    ax.set_yticks(range(len(labels)))\n",
        "    ax.set_yticklabels(labels, fontsize=9)\n",
        "    ax.set_xlabel('Score', fontsize=10)\n",
        "    ax.set_title(title, fontsize=11, fontweight='bold')\n",
        "    ax.set_xlim(0, 5)\n",
        "    ax.grid(axis='x', alpha=0.3)\n",
        "    \n",
        "    for i, (bar, val) in enumerate(zip(bars, plot_data)):\n",
        "        ax.text(val + 0.05, i, f'{val:.2f}', va='center', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15. Temperature Impact Analysis\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "temp_analysis = runs[runs['params.llm_name'].str.contains('claude-sonnet', na=False)].copy()\n",
        "\n",
        "if len(temp_analysis) > 0:\n",
        "    temp_analysis = temp_analysis[[\n",
        "        'params.llm_name',\n",
        "        'params.temperature',\n",
        "        'metrics.avg_overall_score',\n",
        "        'metrics.avg_accuracy',\n",
        "        'metrics.avg_clarity',\n",
        "        'metrics.std_overall_score'\n",
        "    ]].sort_values('params.temperature')\n",
        "    \n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    temps = temp_analysis['params.temperature'].astype(float)\n",
        "    scores = temp_analysis['metrics.avg_overall_score']\n",
        "    std_devs = temp_analysis['metrics.std_overall_score']\n",
        "    \n",
        "    ax1.errorbar(temps, scores, yerr=std_devs, marker='o', markersize=8, \n",
        "                 capsize=5, linewidth=2, color='steelblue')\n",
        "    ax1.set_xlabel('Temperature', fontsize=12, fontweight='bold')\n",
        "    ax1.set_ylabel('Average Overall Score', fontsize=12, fontweight='bold')\n",
        "    ax1.set_title('Impact of Temperature on Performance', fontsize=13, fontweight='bold')\n",
        "    ax1.set_ylim(0, 5.5)\n",
        "    ax1.grid(alpha=0.3)\n",
        "    \n",
        "    accuracy = temp_analysis['metrics.avg_accuracy']\n",
        "    clarity = temp_analysis['metrics.avg_clarity']\n",
        "    \n",
        "    scatter = ax2.scatter(accuracy, clarity, c=temps, s=200, cmap='coolwarm', \n",
        "                         edgecolors='black', linewidths=1.5, alpha=0.7)\n",
        "    \n",
        "    for temp, acc, clar in zip(temps, accuracy, clarity):\n",
        "        ax2.annotate(f'T={temp:.1f}', (acc, clar), fontsize=9, \n",
        "                    ha='center', va='bottom', fontweight='bold')\n",
        "    \n",
        "    ax2.set_xlabel('Accuracy Score', fontsize=12, fontweight='bold')\n",
        "    ax2.set_ylabel('Clarity Score', fontsize=12, fontweight='bold')\n",
        "    ax2.set_title('Accuracy vs Clarity Trade-off', fontsize=13, fontweight='bold')\n",
        "    ax2.grid(alpha=0.3)\n",
        "    \n",
        "    cbar = plt.colorbar(scatter, ax=ax2)\n",
        "    cbar.set_label('Temperature', fontsize=10, fontweight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Not enough data for temperature analysis\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 16. Summary Length Analysis\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "models = comparison_metrics['params.llm_name'].values\n",
        "lengths = comparison_metrics['metrics.avg_summary_length'].values\n",
        "\n",
        "bars = ax1.barh(models, lengths, color='coral', alpha=0.8)\n",
        "for i, (bar, length) in enumerate(zip(bars, lengths)):\n",
        "    ax1.text(length + 50, i, f'{length:.0f}', va='center', fontweight='bold')\n",
        "\n",
        "ax1.set_xlabel('Average Summary Length (characters)', fontsize=12, fontweight='bold')\n",
        "ax1.set_ylabel('LLM Configuration', fontsize=12, fontweight='bold')\n",
        "ax1.set_title('Average Summary Length by Model', fontsize=13, fontweight='bold')\n",
        "ax1.grid(axis='x', alpha=0.3)\n",
        "\n",
        "ax2.scatter(comparison_metrics['metrics.avg_summary_length'],\n",
        "           comparison_metrics['metrics.avg_overall_score'],\n",
        "           s=150, alpha=0.6, color='steelblue', edgecolors='black', linewidths=1.5)\n",
        "\n",
        "for _, row in comparison_metrics.iterrows():\n",
        "    ax2.annotate(row['params.llm_name'], \n",
        "                (row['metrics.avg_summary_length'], row['metrics.avg_overall_score']),\n",
        "                fontsize=8, ha='left', va='bottom')\n",
        "\n",
        "ax2.set_xlabel('Average Summary Length (characters)', fontsize=12, fontweight='bold')\n",
        "ax2.set_ylabel('Average Overall Score', fontsize=12, fontweight='bold')\n",
        "ax2.set_title('Length vs Quality Trade-off', fontsize=13, fontweight='bold')\n",
        "ax2.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 17. Winner Declaration and Recommendations\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "best_model = comparison_metrics.iloc[0]\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"EXPERIMENT RESULTS SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "print(f\"WINNER: {best_model['params.llm_name']}\")\n",
        "print(f\"Overall Score: {best_model['metrics.avg_overall_score']:.3f}/5.0\")\n",
        "print(f\"Temperature: {best_model['params.temperature']}\")\n",
        "print()\n",
        "print(\"Breakdown:\")\n",
        "print(f\"  Accuracy:     {best_model['metrics.avg_accuracy']:.2f}/5.0\")\n",
        "print(f\"  Completeness: {best_model['metrics.avg_completeness']:.2f}/5.0\")\n",
        "print(f\"  Chronology:   {best_model['metrics.avg_chronology']:.2f}/5.0\")\n",
        "print(f\"  Clarity:      {best_model['metrics.avg_clarity']:.2f}/5.0\")\n",
        "print(f\"  Conciseness:  {best_model['metrics.avg_conciseness']:.2f}/5.0\")\n",
        "print()\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "print(\"RECOMMENDATIONS:\")\n",
        "print()\n",
        "\n",
        "best_accuracy = comparison_metrics.nlargest(1, 'metrics.avg_accuracy').iloc[0]\n",
        "print(f\"For maximum accuracy: {best_accuracy['params.llm_name']}\")\n",
        "print(f\"  (Accuracy score: {best_accuracy['metrics.avg_accuracy']:.2f})\")\n",
        "print()\n",
        "\n",
        "best_clarity = comparison_metrics.nlargest(1, 'metrics.avg_clarity').iloc[0]\n",
        "print(f\"For best clarity: {best_clarity['params.llm_name']}\")\n",
        "print(f\"  (Clarity score: {best_clarity['metrics.avg_clarity']:.2f})\")\n",
        "print()\n",
        "\n",
        "best_concise = comparison_metrics.nlargest(1, 'metrics.avg_conciseness').iloc[0]\n",
        "print(f\"For conciseness: {best_concise['params.llm_name']}\")\n",
        "print(f\"  (Conciseness score: {best_concise['metrics.avg_conciseness']:.2f})\")\n",
        "print()\n",
        "\n",
        "print(f\"For overall balance: {best_model['params.llm_name']}\")\n",
        "print()\n",
        "print(\"=\"*80)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 18. Judge Agreement Analysis\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "# Analyze judge agreement across models\n",
        "if 'metrics.avg_judge_agreement' in comparison_metrics.columns:\n",
        "    \n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # Plot 1: Judge Agreement by Model\n",
        "    models = comparison_metrics['params.llm_name'].values\n",
        "    agreements = comparison_metrics['metrics.avg_judge_agreement'].values\n",
        "    \n",
        "    bars = ax1.barh(models, agreements, color='mediumseagreen', alpha=0.8)\n",
        "    for i, (bar, agree) in enumerate(zip(bars, agreements)):\n",
        "        ax1.text(agree + 0.01, i, f'{agree:.1%}', va='center', fontweight='bold')\n",
        "    \n",
        "    ax1.set_xlabel('Average Judge Agreement', fontsize=12, fontweight='bold')\n",
        "    ax1.set_ylabel('LLM Configuration', fontsize=12, fontweight='bold')\n",
        "    ax1.set_title('Judge Consensus by Model', fontsize=13, fontweight='bold')\n",
        "    ax1.set_xlim(0, 1.1)\n",
        "    ax1.grid(axis='x', alpha=0.3)\n",
        "    ax1.axvline(x=0.8, color='red', linestyle='--', alpha=0.5, label='High Agreement Threshold')\n",
        "    ax1.legend()\n",
        "    \n",
        "    # Plot 2: Overall Score vs Judge Agreement\n",
        "    ax2.scatter(comparison_metrics['metrics.avg_overall_score'],\n",
        "               comparison_metrics['metrics.avg_judge_agreement'],\n",
        "               s=150, alpha=0.6, color='mediumseagreen', edgecolors='black', linewidths=1.5)\n",
        "    \n",
        "    for _, row in comparison_metrics.iterrows():\n",
        "        ax2.annotate(row['params.llm_name'], \n",
        "                    (row['metrics.avg_overall_score'], row['metrics.avg_judge_agreement']),\n",
        "                    fontsize=8, ha='left', va='bottom')\n",
        "    \n",
        "    ax2.set_xlabel('Average Overall Score', fontsize=12, fontweight='bold')\n",
        "    ax2.set_ylabel('Judge Agreement', fontsize=12, fontweight='bold')\n",
        "    ax2.set_title('Quality vs Judge Consensus', fontsize=13, fontweight='bold')\n",
        "    ax2.grid(alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\nJudge Agreement Interpretation:\")\n",
        "    print(\"  > 90%: Very high consensus - all judges strongly agree\")\n",
        "    print(\"  80-90%: High consensus - judges mostly agree\")\n",
        "    print(\"  70-80%: Moderate consensus - some disagreement\")\n",
        "    print(\"  < 70%: Low consensus - significant judge disagreement\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 19. Individual Judge Performance Comparison\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "# Load judge breakdown data from the best run\n",
        "best_run_id = runs.iloc[0]['run_id']\n",
        "\n",
        "try:\n",
        "    judge_breakdown_path = client.download_artifacts(best_run_id, \"judge_breakdown.csv\")\n",
        "    judge_breakdown_df = pd.read_csv(judge_breakdown_path)\n",
        "    \n",
        "    # Calculate average scores per judge\n",
        "    judge_summary = judge_breakdown_df.groupby('judge').agg({\n",
        "        'accuracy': 'mean',\n",
        "        'completeness': 'mean',\n",
        "        'chronology': 'mean',\n",
        "        'clarity': 'mean',\n",
        "        'conciseness': 'mean',\n",
        "        'overall_score': 'mean'\n",
        "    }).round(2)\n",
        "    \n",
        "    print(\"=\"*80)\n",
        "    print(\"AVERAGE SCORES BY JUDGE\")\n",
        "    print(\"=\"*80)\n",
        "    display(judge_summary)\n",
        "    \n",
        "    # Visualize judge comparison\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    \n",
        "    judge_summary.plot(kind='bar', ax=ax, alpha=0.8)\n",
        "    ax.set_xlabel('Judge', fontsize=12, fontweight='bold')\n",
        "    ax.set_ylabel('Average Score', fontsize=12, fontweight='bold')\n",
        "    ax.set_title('Score Distribution by Judge', fontsize=13, fontweight='bold')\n",
        "    ax.set_ylim(0, 5.5)\n",
        "    ax.legend(title='Metric', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    ax.grid(axis='y', alpha=0.3)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Analyze judge strictness\n",
        "    judge_strictness = judge_breakdown_df.groupby('judge')['overall_score'].agg(['mean', 'std']).round(2)\n",
        "    judge_strictness.columns = ['Mean Score', 'Std Dev']\n",
        "    judge_strictness['Strictness'] = 5.0 - judge_strictness['Mean Score']  # Higher = stricter\n",
        "    judge_strictness = judge_strictness.sort_values('Strictness', ascending=False)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"JUDGE STRICTNESS ANALYSIS\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"(Lower mean score = stricter judge)\")\n",
        "    display(judge_strictness)\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Could not load judge breakdown data: {e}\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 20. Export Results\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "report_df = comparison_metrics.copy()\n",
        "report_df.columns = [col.replace('params.', '').replace('metrics.', '') for col in report_df.columns]\n",
        "\n",
        "spark.createDataFrame(report_df).write.mode(\"overwrite\").saveAsTable(\"rag_experiment_results\")\n",
        "\n",
        "print(\"Results saved to Delta table: rag_experiment_results\")\n",
        "print()\n",
        "print(\"Query with:\")\n",
        "print(\"  SELECT * FROM rag_experiment_results ORDER BY avg_overall_score DESC\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 21. Sample Summaries Comparison\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "def display_sample_summaries(complaint_id: str):\n",
        "    \"\"\"Display summaries from all models for a given complaint.\"\"\"\n",
        "    \n",
        "    print(f\"\\n{'='*100}\")\n",
        "    print(f\"COMPLAINT ID: {complaint_id}\")\n",
        "    print(f\"{'='*100}\\n\")\n",
        "    \n",
        "    for _, row in comparison_metrics.iterrows():\n",
        "        run_id = runs[runs['params.llm_name'] == row['params.llm_name']].iloc[0]['run_id']\n",
        "        \n",
        "        try:\n",
        "            summaries_path = client.download_artifacts(run_id, \"summaries.csv\")\n",
        "            summaries_df = pd.read_csv(summaries_path)\n",
        "            \n",
        "            summary_row = summaries_df[summaries_df['complaint_id'] == complaint_id]\n",
        "            \n",
        "            if not summary_row.empty:\n",
        "                summary = summary_row.iloc[0]['summary']\n",
        "                model_name = row['params.llm_name']\n",
        "                score = row['metrics.avg_overall_score']\n",
        "                \n",
        "                print(f\"{model_name} (Score: {score:.2f})\")\n",
        "                print(f\"{'-'*100}\")\n",
        "                print(summary)\n",
        "                print(f\"\\n\")\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "# Display sample summaries for first test complaint\n",
        "test_complaint_id = \"CDCR-23502631\"\n",
        "display_sample_summaries(test_complaint_id)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook provides a complete RAG system with MLflow experiment tracking:\n",
        "\n",
        "1. Data loading and preprocessing\n",
        "2. Document chunking with section preservation\n",
        "3. Embedding generation using Databricks endpoints\n",
        "4. Vector search for similarity retrieval\n",
        "5. LLM integration for summarization\n",
        "6. Multi-judge LLM evaluation framework (all models evaluate each other)\n",
        "7. MLflow experiment tracking with judge agreement metrics\n",
        "8. Comprehensive visualization and analysis\n",
        "9. Judge consensus and strictness analysis\n",
        "10. Results export and recommendations\n",
        "\n",
        "**Key Innovation:** Using multiple LLMs as judges provides:\n",
        "- More robust evaluation through averaging\n",
        "- Judge agreement metrics to assess confidence\n",
        "- Insights into which models are stricter/more lenient\n",
        "- Reduced bias from any single judge\n",
        "\n",
        "The framework enables systematic comparison of different LLMs, temperatures, and RAG parameters to identify the optimal configuration for complaint summarization.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}